{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thanks to Daniel Villaveces' article on *Medium* for helping me to scrape the data from Citi Bike data. \n",
    "#### It saved me a lot of time that otherwise would have been spent downloading multiple zip files for each month.\n",
    "#### Daniel's article can be found [here](https://medium.com/@dvillaveces/analyzing-citi-bike-data-2202f9da97d7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the citi bike index and extract the file names\n",
    "# the data begins in 2013, but I only want data from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the files also have a second file named with 'JC' for every month from 2015 - 2020 that seems to contain addtional/\n",
    "# trip data, so I will have to do this twice: first to get the 'non JC files' and then get the'JC' files\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202001-citibike-tripdata.csv.zip',\n",
       " '202002-citibike-tripdata.csv.zip',\n",
       " '202003-citibike-tripdata.csv.zip',\n",
       " '202004-citibike-tripdata.csv.zip',\n",
       " '202005-citibike-tripdata.csv.zip',\n",
       " '202006-citibike-tripdata.csv.zip',\n",
       " '202007-citibike-tripdata.csv.zip',\n",
       " '202008-citibike-tripdata.csv.zip',\n",
       " '202009-citibike-tripdata.csv.zip',\n",
       " '202010-citibike-tripdata.csv.zip',\n",
       " '202011-citibike-tripdata.csv.zip',\n",
       " '202012-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://s3.amazonaws.com/tripdata/'\n",
    "\n",
    "# load all url content into soup\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'xml')\n",
    "\n",
    "# extract file names from soup\n",
    "# data from 2020\n",
    "files = soup.find_all('Key')\n",
    "extract_files = []\n",
    "for i in range(80, 92):\n",
    "    extract_files.append(files[i].get_text())\n",
    "    \n",
    "extract_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JC-202001-citibike-tripdata.csv.zip',\n",
       " 'JC-202002-citibike-tripdata.csv.zip',\n",
       " 'JC-202003-citibike-tripdata.csv.zip',\n",
       " 'JC-202004-citibike-tripdata.csv.zip',\n",
       " 'JC-202005-citibike-tripdata.csv.zip',\n",
       " 'JC-202006-citibike-tripdata.csv.zip',\n",
       " 'JC-202007-citibike-tripdata.csv.zip',\n",
       " 'JC-202008-citibike-tripdata.csv.zip',\n",
       " 'JC-202009-citibike-tripdata.csv.zip',\n",
       " 'JC-202010-citibike-tripdata.csv.zip',\n",
       " 'JC-202011-citibike-tripdata.csv.zip',\n",
       " 'JC-202012-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the JC files, 2020\n",
    "files = soup.find_all('Key')\n",
    "jc_extract_files = []\n",
    "for i in range(145, len(files)-2):\n",
    "    jc_extract_files.append(files[i].get_text())\n",
    "    \n",
    "jc_extract_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip the files using a for loop\n",
    "\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & unzip non-JC files\n",
    "# move data up a directory so it's not added to github\n",
    "\n",
    "for file in extract_files:\n",
    "    file_url = url + file\n",
    "    \n",
    "    #download files\n",
    "    with open(file, \"wb\") as f:\n",
    "            response = requests.get(file_url)\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # unzip data files\n",
    "    with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"../tripdata\")        \n",
    "    \n",
    "    # remove zipped file after unziping\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & unzip JC files\n",
    "\n",
    "for file in jc_extract_files:\n",
    "    file_url = url + file\n",
    "    \n",
    "    #download files\n",
    "    with open(file, \"wb\") as f:\n",
    "            response = requests.get(file_url)\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # unzip data files\n",
    "    with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"../tripdata\")        \n",
    "    \n",
    "    # remove zipped file after unziping\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename files to be more manageable\n",
    "# files have long names such as '202001-citibike-tripdata.csv' or 'JC-202001-citibike-tripdata'\n",
    "# just want the year and month and also include the 'jc' until I can figure out the difference\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../tripdata/'\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.csv'):\n",
    "        new_filename = filename.replace(' ','').lower().split('ci', 1)[0].strip('-').replace('-','_')\n",
    "        os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into DataFrames into dfs dictionary\n",
    "directory = '../tripdata/'\n",
    "dfs = {}\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.csv'):\n",
    "        dfs[filename.split('.')[0]] = pd.read_csv(os.path.join(directory, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
